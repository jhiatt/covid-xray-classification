{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import random as python_random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.random.seed(42) # note that you must use the same seed to ensure consistentcy in your training/validation/testing\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "#### Questions:\n",
    "# How does tf (under this method) store labels? Can I access them?\n",
    "#    Look into how the resizing line is done\n",
    "# What happens if I don't pass the y value into imageDataGenerator().flow()?\n",
    "# is this enough work? (oversampling, data augmentation, adusting the prediction wieghts)\n",
    "# multiple expert -  3 different model (majority voting at the end to make predictions) - cost trade off for computing\n",
    "# - talk about costs (training time, different work, etc)\n",
    "# - accuracy vs tradeoffs (latency of making one prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " covid-xray-classification   dl_data_2\t\t     resnet_v2.ipynb\n",
      " dl_data\t\t     dl_load.ipynb\t     saved_models\n",
      " dl_data.zip\t\t    'jordan_cnn (2).ipynb'\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/qumulo/qhome/hso6b/DL\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "covidDataPath='dl_data_2/dl_data/Covid_img'\n",
    "pneumoniaDataPath='dl_data_2/dl_data/Viral_img'\n",
    "normalDataPath='dl_data_2/dl_data/Normal_img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for access paths\n",
    "listCovidPaths = []\n",
    "listViralPaths=[]\n",
    "listNormalPaths = []\n",
    "\n",
    "# Get covid images files paths\n",
    "for root, directories, files in os.walk(covidDataPath):\n",
    "    for name in files:\n",
    "        listCovidPaths.append(os.path.join(root, name))\n",
    "\n",
    "for root, directories, files in os.walk(pneumoniaDataPath):\n",
    "    for name in files:\n",
    "        listViralPaths.append(os.path.join(root, name))        \n",
    "        \n",
    "# Get normal images files paths\n",
    "for root, directories, files in os.walk(normalDataPath):\n",
    "    for name in files:\n",
    "        listNormalPaths.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dl_data_2/dl_data/'\n",
    "class_names = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Covid_img', '.ipynb_checkpoints', 'Viral_img', 'Normal_img']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.ipynb_checkpoints'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Covid_img', 'Viral_img', 'Normal_img']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = class_names[-3:]\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid_img\n",
      "3610\n",
      "Viral_img\n",
      "1345\n",
      "Normal_img\n",
      "10192\n"
     ]
    }
   ],
   "source": [
    "class_dist = {}\n",
    "for c in class_names:\n",
    "    class_dist[c] = len(os.listdir(data_path + c))\n",
    "    print(c)\n",
    "    print(class_dist[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pull_val(dir_path, val_path, portion):\n",
    "    # get a list of files to hold out for validation\n",
    "    files = os.listdir(dir_path)\n",
    "    length = len(files)\n",
    "    num_files = math.floor(length * portion)\n",
    "    val_files = np.random.choice(files, size=num_files, replace=False)\n",
    "\n",
    "    # move files\n",
    "    for f in val_files:\n",
    "        os.rename(dir_path + '/' + f, val_path + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to, use this to make a new directory\n",
    "\n",
    "os.mkdir('dl_data_2/HOLD_data')\n",
    "os.mkdir('dl_data_2/test_data')\n",
    "for c in class_names:\n",
    "    os.mkdir('dl_data_2/HOLD_data/' + c)\n",
    "    os.mkdir('dl_data_2/test_data/' + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out holdout and testing data\n",
    "\n",
    "portion = .10 # portion of data set aside for HOLDOUT\n",
    "    \n",
    "for c in class_names:\n",
    "    dir_path = 'dl_data_2/dl_data/' + c\n",
    "    val_path = 'dl_data_2/HOLD_data/' + c\n",
    "    pull_val(dir_path, val_path, portion)\n",
    "    \n",
    "##### Commented out because we will do validation split with the image gen function    \n",
    "# portion = .20 # portion of data set aside for TESTING\n",
    "    \n",
    "# for c in class_names:\n",
    "#     dir_path = '../dl_data/' + c\n",
    "#     val_path = '../test_data/' + c\n",
    "#     pull_val(dir_path, val_path, portion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10908 images belonging to 3 classes.\n",
      "Found 2725 images belonging to 3 classes.\n",
      "Found 1514 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files \n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#### calculate class weights\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "total = sum(class_dist.values())\n",
    "weight_for_0 = (1 / class_dist[class_names[0]]) * (total / 2.0)\n",
    "weight_for_1 = (1 / class_dist[class_names[2]]) * (total / 2.0)\n",
    "weight_for_2 = (1 / class_dist[class_names[3]]) * (total / 2.0)\n",
    "\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "\n",
    "# directories\n",
    "data_dir = \"dl_data_2/dl_data\"\n",
    "# test_dir = \"../test_data\"\n",
    "HOLD_dir = \"dl_data_2/HOLD_data\"\n",
    "\n",
    "\n",
    "#### save out augmented data for visualization\n",
    "\n",
    "# ## first delete any existing files\n",
    "# aug_dir = '../augmented_data'\n",
    "# aug_files = os.listdir(aug_dir)\n",
    "# for f in aug_files:\n",
    "#     os.remove(aug_dir + '/' + f)\n",
    "\n",
    "    \n",
    "batch_size = 32;\n",
    "# IMPORTANT: Depends on what pre-trained model you choose, you will need to change these dimensions accordingly\n",
    "img_height = 224; \n",
    "img_width = 224;\n",
    "    \n",
    "\n",
    "# data augmentation (for training only)\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255,\n",
    "                                    zoom_range= 0.3, \n",
    "                                    horizontal_flip= True, \n",
    "                                    shear_range= 0.2,\n",
    "                                    rotation_range = 30,\n",
    "                                    validation_split=0.2\n",
    "                                    \n",
    "                                    \n",
    "#                                     featurewise_center=False,\n",
    "#                                     samplewise_center=False,\n",
    "#                                     featurewise_std_normalization=False,\n",
    "#                                     samplewise_std_normalization=False,\n",
    "#                                     zca_whitening=False,\n",
    "#                                     zca_epsilon=1e-06,\n",
    "#                                     rotation_range=0,\n",
    "#                                     width_shift_range=0.0,\n",
    "#                                     height_shift_range=0.0,\n",
    "#                                     brightness_range=None,\n",
    "#                                     shear_range=0.0,\n",
    "#                                     zoom_range=0.0,\n",
    "#                                     channel_shift_range=0.0,\n",
    "#                                     fill_mode='nearest',\n",
    "#                                     cval=0.0,\n",
    "#                                     horizontal_flip=False,\n",
    "#                                     vertical_flip=False,\n",
    "#                                     rescale=None,\n",
    "#                                     preprocessing_function=None,\n",
    "#                                     data_format=None,\n",
    "#                                     validation_split=0.2,\n",
    "#                                     dtype=None\n",
    "                                    )\n",
    "\n",
    "\n",
    "train_ds = train_data_gen.flow_from_directory(\n",
    "    directory = data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "#     save_to_dir=aug_dir,\n",
    "#     save_prefix='aug',\n",
    "#     save_format='png',\n",
    "    follow_links=False,\n",
    "    subset='training',\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "validation_ds = train_data_gen.flow_from_directory(\n",
    "    directory=data_dir,  # same directory because we are splitting the data here\n",
    "    follow_links=False,\n",
    "    subset='validation',\n",
    "    interpolation='nearest',\n",
    "    target_size=(img_height, img_width), \n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# holdout data\n",
    "HOLD_ds = test_data_gen.flow_from_directory(directory=HOLD_dir, \n",
    "                                         target_size=(img_height, img_width), \n",
    "                                         class_mode='categorical',\n",
    "                                         shuffle=True,\n",
    "                                         seed=42,\n",
    "                                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint to resume training if it stops unexpectedly\n",
    "checkpoint_path = \"../checkpoints/training_2/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 13:18:33.884781: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-22 13:18:36.317212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10788 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "ds_size_1 = (224, 224)\n",
    "# train_ds_1 = train_ds.map(lambda image, label: (tf.image.resize(image, ds_size_1), label))\n",
    "# validation_ds_1 = validation_ds.map(lambda image, label: (tf.image.resize(image, ds_size_1), label))\n",
    "\n",
    "train_ds_1 = train_ds\n",
    "validation_ds_1 = validation_ds\n",
    "\n",
    "\n",
    "base_model_2 = keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "n_classes = 3\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# Rebuild top\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model_2.output)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "top_dropout_rate = 0.2\n",
    "x = tf.keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "# x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(3, activation=\"softmax\", name=\"pred\")(x) # match number of classes\n",
    "\n",
    "model_2 = keras.models.Model(inputs=base_model_2.input,\n",
    "                           outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cp_callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      7\u001b[0m model_2\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      8\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m history \u001b[38;5;241m=\u001b[39m model_2\u001b[38;5;241m.\u001b[39mfit(train_ds_1,\n\u001b[1;32m     10\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39mvalidation_ds_1,\n\u001b[1;32m     11\u001b[0m                     class_weight\u001b[38;5;241m=\u001b[39mclass_weights,\n\u001b[0;32m---> 12\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[callback,\u001b[43mcp_callback\u001b[49m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cp_callback' is not defined"
     ]
    }
   ],
   "source": [
    "# train up the top layer first\n",
    "\n",
    "for layer in base_model_2.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01, decay=0.01)\n",
    "model_2.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model_2.fit(train_ds_1,\n",
    "                    validation_data=validation_ds_1,\n",
    "                    class_weight=class_weights,\n",
    "                    epochs=20, callbacks=[callback,cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 0s 0us/step\n",
      "80150528/80134624 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model_1 = keras.applications.VGG19(weights='imagenet', include_top=False)\n",
    "n_classes = 3\n",
    "\n",
    "# Rebuild top\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model_1.output)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "top_dropout_rate = 0.2\n",
    "x = tf.keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "# x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(3, activation=\"softmax\", name=\"pred\")(x) # match number of classes\n",
    "\n",
    "model_1 = keras.models.Model(inputs=base_model_1.input,\n",
    "                           outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "341/341 [==============================] - 164s 470ms/step - loss: 1.4631 - accuracy: 0.7515 - val_loss: 0.9029 - val_accuracy: 0.6730\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 161s 471ms/step - loss: 1.2356 - accuracy: 0.7804 - val_loss: 0.5148 - val_accuracy: 0.7831\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 161s 470ms/step - loss: 1.1943 - accuracy: 0.7845 - val_loss: 0.4261 - val_accuracy: 0.8371\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 160s 469ms/step - loss: 1.1652 - accuracy: 0.7861 - val_loss: 0.4644 - val_accuracy: 0.8092\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 161s 470ms/step - loss: 1.1841 - accuracy: 0.7868 - val_loss: 0.4226 - val_accuracy: 0.8257\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 161s 472ms/step - loss: 1.1638 - accuracy: 0.7911 - val_loss: 0.4131 - val_accuracy: 0.8349\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 161s 472ms/step - loss: 1.1415 - accuracy: 0.7968 - val_loss: 0.4280 - val_accuracy: 0.8242\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 161s 472ms/step - loss: 1.1495 - accuracy: 0.7900 - val_loss: 0.4331 - val_accuracy: 0.8206\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 163s 479ms/step - loss: 1.1527 - accuracy: 0.7942 - val_loss: 0.4300 - val_accuracy: 0.8272\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 161s 473ms/step - loss: 1.1436 - accuracy: 0.7975 - val_loss: 0.4383 - val_accuracy: 0.8220\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 163s 476ms/step - loss: 1.1288 - accuracy: 0.7962 - val_loss: 0.4487 - val_accuracy: 0.8150\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 162s 475ms/step - loss: 1.1271 - accuracy: 0.7977 - val_loss: 0.4337 - val_accuracy: 0.8334\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 163s 477ms/step - loss: 1.1417 - accuracy: 0.7968 - val_loss: 0.4400 - val_accuracy: 0.8191\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 162s 476ms/step - loss: 1.1335 - accuracy: 0.7957 - val_loss: 0.4210 - val_accuracy: 0.8279\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 162s 473ms/step - loss: 1.1415 - accuracy: 0.7957 - val_loss: 0.4277 - val_accuracy: 0.8257\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 163s 476ms/step - loss: 1.1261 - accuracy: 0.7983 - val_loss: 0.4355 - val_accuracy: 0.8242\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 163s 478ms/step - loss: 1.1386 - accuracy: 0.7971 - val_loss: 0.4059 - val_accuracy: 0.8308\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 162s 474ms/step - loss: 1.1429 - accuracy: 0.7982 - val_loss: 0.4444 - val_accuracy: 0.8202\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 161s 473ms/step - loss: 1.1374 - accuracy: 0.7934 - val_loss: 0.3928 - val_accuracy: 0.8437\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 162s 474ms/step - loss: 1.1550 - accuracy: 0.7956 - val_loss: 0.4190 - val_accuracy: 0.8294\n"
     ]
    }
   ],
   "source": [
    "# train up the top layer first\n",
    "\n",
    "for layer in base_model_1.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01, decay=0.01)\n",
    "model_1.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model_1.fit(train_ds_1,\n",
    "                    validation_data=validation_ds_1,\n",
    "                    class_weight=class_weights,\n",
    "                    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "341/341 [==============================] - 332s 969ms/step - loss: 1.3782 - accuracy: 0.7520 - val_loss: 3.2209 - val_accuracy: 0.7982\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 330s 968ms/step - loss: 1.1989 - accuracy: 0.7902 - val_loss: 0.3776 - val_accuracy: 0.8653\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 331s 969ms/step - loss: 1.0791 - accuracy: 0.8108 - val_loss: 1.2438 - val_accuracy: 0.7163\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 331s 969ms/step - loss: 0.9545 - accuracy: 0.8398 - val_loss: 16.9430 - val_accuracy: 0.8308\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 331s 970ms/step - loss: 0.9304 - accuracy: 0.8400 - val_loss: 0.5204 - val_accuracy: 0.8407\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 331s 970ms/step - loss: 0.8469 - accuracy: 0.8575 - val_loss: 2.1190 - val_accuracy: 0.7020\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 331s 969ms/step - loss: 0.8271 - accuracy: 0.8601 - val_loss: 0.3703 - val_accuracy: 0.8569\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 331s 969ms/step - loss: 0.7249 - accuracy: 0.8763 - val_loss: 0.3896 - val_accuracy: 0.8470\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 331s 970ms/step - loss: 0.6810 - accuracy: 0.8877 - val_loss: 0.5655 - val_accuracy: 0.8334\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 331s 969ms/step - loss: 0.6495 - accuracy: 0.8944 - val_loss: 0.6075 - val_accuracy: 0.8360\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 331s 971ms/step - loss: 0.6569 - accuracy: 0.8903 - val_loss: 0.4201 - val_accuracy: 0.8727\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 332s 973ms/step - loss: 0.5779 - accuracy: 0.9016 - val_loss: 59.3011 - val_accuracy: 0.4356\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 333s 977ms/step - loss: 0.5642 - accuracy: 0.9057 - val_loss: 0.4087 - val_accuracy: 0.8473\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 333s 977ms/step - loss: 0.5329 - accuracy: 0.9102 - val_loss: 0.5184 - val_accuracy: 0.8404\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 332s 972ms/step - loss: 0.5323 - accuracy: 0.9101 - val_loss: 0.7995 - val_accuracy: 0.6873\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 332s 973ms/step - loss: 0.5006 - accuracy: 0.9186 - val_loss: 0.2548 - val_accuracy: 0.9134\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 331s 970ms/step - loss: 0.4968 - accuracy: 0.9169 - val_loss: 0.2330 - val_accuracy: 0.9200\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 331s 971ms/step - loss: 0.4661 - accuracy: 0.9241 - val_loss: 0.5401 - val_accuracy: 0.8462\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 331s 970ms/step - loss: 0.4429 - accuracy: 0.9246 - val_loss: 0.4715 - val_accuracy: 0.8613\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 331s 971ms/step - loss: 0.4534 - accuracy: 0.9258 - val_loss: 0.1791 - val_accuracy: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# train all the layers together for a bit with a much lower learning rate\n",
    "\n",
    "for layer in base_model_1.layers[-20:]:\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005, decay=0.001)\n",
    "model_1.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model_1.fit(train_ds_1,\n",
    "                    validation_data=validation_ds_1,\n",
    "                    class_weight=class_weights,\n",
    "                    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:36:09.807271: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/model_VGG/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model_1.save('saved_models/model_VGG') # change this path to save a new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.8.0/Keras Py3.9",
   "language": "python",
   "name": "tensorflow-2.8.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
