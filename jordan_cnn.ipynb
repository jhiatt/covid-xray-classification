{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import random as python_random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.random.seed(42) # note that you must use the same seed to ensure consistentcy in your training/validation/testing\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "#### Questions:\n",
    "# How does tf (under this method) store labels? Can I access them?\n",
    "#    Look into how the resizing line is done\n",
    "# What happens if I don't pass the y value into imageDataGenerator().flow()?\n",
    "# is this enough work? (oversampling, data augmentation, adusting the prediction wieghts)\n",
    "# multiple expert -  3 different model (majority voting at the end to make predictions) - cost trade off for computing\n",
    "# - talk about costs (training time, different work, etc)\n",
    "# - accuracy vs tradeoffs (latency of making one prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t  jordan_cnn-manual-split.ipynb  resnet-jordan.ipynb\n",
      "checkpoints\t  jordan_cnn.ipynb\t\t resnet.ipynb\n",
      "dl_load.ipynb\t  matrix1.png\t\t\t resnet_v2.ipynb\n",
      "evaluation.ipynb  matrix2.png\t\t\t saved_models\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid_img\n",
      "3611\n",
      "Viral_img\n",
      "1345\n",
      "Normal_img\n",
      "10193\n"
     ]
    }
   ],
   "source": [
    "data_path = '../dl_data/'\n",
    "class_names = os.listdir(data_path)\n",
    "class_dist = {}\n",
    "for c in class_names:\n",
    "    class_dist[c] = len(os.listdir(data_path + c))\n",
    "    print(c)\n",
    "    print(class_dist[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pull_val(dir_path, val_path, portion):\n",
    "    # get a list of files to hold out for validation\n",
    "    files = os.listdir(dir_path)\n",
    "    length = len(files)\n",
    "    num_files = math.floor(length * portion)\n",
    "    val_files = np.random.choice(files, size=num_files, replace=False)\n",
    "\n",
    "    # move files\n",
    "    for f in val_files:\n",
    "        os.rename(dir_path + '/' + f, val_path + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to, use this to make a new directory\n",
    "\n",
    "# os.mkdir('../HOLD_data')\n",
    "# os.mkdir('../test_data')\n",
    "# for c in class_names:\n",
    "#     os.mkdir('../HOLD_data/' + c)\n",
    "#     os.mkdir('../test_data/' + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out holdout and testing data\n",
    "\n",
    "portion = .10 # portion of data set aside for HOLDOUT\n",
    "    \n",
    "for c in class_names:\n",
    "    dir_path = '../dl_data/' + c\n",
    "    val_path = '../HOLD_data/' + c\n",
    "    pull_val(dir_path, val_path, portion)\n",
    "    \n",
    "##### Commented out because we will do validation split with the image gen function    \n",
    "# portion = .20 # portion of data set aside for TESTING\n",
    "    \n",
    "# for c in class_names:\n",
    "#     dir_path = '../dl_data/' + c\n",
    "#     val_path = '../test_data/' + c\n",
    "#     pull_val(dir_path, val_path, portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reset holdout\n",
    "\n",
    "# for c in class_names:\n",
    "#     val_files = os.listdir('../HOLD_data/' + c)\n",
    "#     for i in val_files:\n",
    "#         os.rename('../HOLD_data/' + c + '/' + i, '../dl_data/' + c + '/' + i)\n",
    "        \n",
    "# for c in class_names:\n",
    "#     val_files = os.listdir('../test_data/' + c)\n",
    "#     for i in val_files:\n",
    "#         os.rename('../test_data/' + c + '/' + i, '../dl_data/' + c + '/' + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Train set-------------------\n",
      "Covid_img\n",
      "3250\n",
      "Viral_img\n",
      "1211\n",
      "Normal_img\n",
      "9174\n",
      "---Teset set-------------------\n",
      "Covid_img\n",
      "0\n",
      "Viral_img\n",
      "0\n",
      "Normal_img\n",
      "0\n",
      "---Holdout set-------------------\n",
      "Covid_img\n",
      "361\n",
      "Viral_img\n",
      "134\n",
      "Normal_img\n",
      "1019\n",
      "---Total-------------------\n",
      "Covid_img\n",
      "3611\n",
      "Viral_img\n",
      "1345\n",
      "Normal_img\n",
      "10193\n"
     ]
    }
   ],
   "source": [
    "print('---Train set-------------------')\n",
    "for c in class_names:\n",
    "    print(c)\n",
    "    print(len(os.listdir('../dl_data/'+c)))\n",
    "    \n",
    "print('---Teset set-------------------')\n",
    "for c in class_names:\n",
    "    print(c)\n",
    "    print(len(os.listdir('../test_data/'+c)))\n",
    "    \n",
    "print('---Holdout set-------------------')\n",
    "for c in class_names:\n",
    "    print(c)\n",
    "    print(len(os.listdir('../HOLD_data/'+c)))\n",
    "    \n",
    "print('---Total-------------------')\n",
    "for c in class_names:\n",
    "    print(c)\n",
    "    print(len(os.listdir('../dl_data/'+c)) + len(os.listdir('../HOLD_data/'+c)) + len(os.listdir('../test_data/'+c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data above isn't run, run this\n",
    "\n",
    "data_path = \"../dl_data/\"\n",
    "class_names = os.listdir(data_path)\n",
    "class_dist = {}\n",
    "for c in class_names:\n",
    "    class_dist[c] = len(os.listdir(data_path + c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Covid_img', 'Viral_img', 'Normal_img']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10912 images belonging to 3 classes.\n",
      "Found 2726 images belonging to 3 classes.\n",
      "Found 1514 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files \n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#### calculate class weights\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "total = sum(class_dist.values())\n",
    "weight_for_0 = (1 / class_dist[class_names[0]]) * (total / 2.0)\n",
    "weight_for_1 = (1 / class_dist[class_names[1]]) * (total / 2.0)\n",
    "weight_for_2 = (1 / class_dist[class_names[2]]) * (total / 2.0)\n",
    "\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "\n",
    "# directories\n",
    "data_dir = \"../dl_data\"\n",
    "# test_dir = \"../test_data\"\n",
    "HOLD_dir = \"../HOLD_data\"\n",
    "\n",
    "\n",
    "#### save out augmented data for visualization\n",
    "\n",
    "# ## first delete any existing files\n",
    "# aug_dir = '../augmented_data'\n",
    "# aug_files = os.listdir(aug_dir)\n",
    "# for f in aug_files:\n",
    "#     os.remove(aug_dir + '/' + f)\n",
    "\n",
    "    \n",
    "batch_size = 32;\n",
    "# IMPORTANT: Depends on what pre-trained model you choose, you will need to change these dimensions accordingly\n",
    "img_height = 224; \n",
    "img_width = 224;\n",
    "    \n",
    "\n",
    "# data augmentation (for training only)\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255,\n",
    "                                    zoom_range= 0.3, \n",
    "                                    horizontal_flip= True, \n",
    "                                    shear_range= 0.2,\n",
    "                                    rotation_range = 30,\n",
    "                                    validation_split=0.2\n",
    "                                    \n",
    "                                    \n",
    "#                                     featurewise_center=False,\n",
    "#                                     samplewise_center=False,\n",
    "#                                     featurewise_std_normalization=False,\n",
    "#                                     samplewise_std_normalization=False,\n",
    "#                                     zca_whitening=False,\n",
    "#                                     zca_epsilon=1e-06,\n",
    "#                                     rotation_range=0,\n",
    "#                                     width_shift_range=0.0,\n",
    "#                                     height_shift_range=0.0,\n",
    "#                                     brightness_range=None,\n",
    "#                                     shear_range=0.0,\n",
    "#                                     zoom_range=0.0,\n",
    "#                                     channel_shift_range=0.0,\n",
    "#                                     fill_mode='nearest',\n",
    "#                                     cval=0.0,\n",
    "#                                     horizontal_flip=False,\n",
    "#                                     vertical_flip=False,\n",
    "#                                     rescale=None,\n",
    "#                                     preprocessing_function=None,\n",
    "#                                     data_format=None,\n",
    "#                                     validation_split=0.2,\n",
    "#                                     dtype=None\n",
    "                                    )\n",
    "\n",
    "\n",
    "train_ds = train_data_gen.flow_from_directory(\n",
    "    directory = data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "#     save_to_dir=aug_dir,\n",
    "#     save_prefix='aug',\n",
    "#     save_format='png',\n",
    "    follow_links=False,\n",
    "    subset='training',\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "validation_ds = train_data_gen.flow_from_directory(\n",
    "    directory=data_dir,  # same directory because we are splitting the data here\n",
    "    follow_links=False,\n",
    "    subset='validation',\n",
    "    interpolation='nearest',\n",
    "    target_size=(img_height, img_width), \n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# holdout data\n",
    "HOLD_ds = test_data_gen.flow_from_directory(directory=HOLD_dir, \n",
    "                                         target_size=(img_height, img_width), \n",
    "                                         class_mode='categorical',\n",
    "                                         shuffle=True,\n",
    "                                         seed=42,\n",
    "                                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is just a bug fix, hopefully I won't need to use it again.\n",
    "\n",
    "# fi = os.listdir(aug_dir + '/' + os.listdir(aug_dir)[0])\n",
    "# for f in fi:\n",
    "#     os.remove(aug_dir + '/' + os.listdir(aug_dir)[0] + '/' + f)\n",
    "\n",
    "# os.rmdir(aug_dir + '/' + os.listdir(aug_dir)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m n_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(n_cols \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, n_rows \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (n_rows\u001b[38;5;241m*\u001b[39mn_cols):\n\u001b[1;32m     11\u001b[0m         plt\u001b[38;5;241m.\u001b[39msubplot(n_rows, n_cols, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'take'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x1728 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some of the train samples of one batch\n",
    "# Make sure you create the class names that match the order of their appearances in the \"files\" variable\n",
    "class_names = ['Covid',  'Normal',  'Viral']\n",
    "\n",
    "# Rows and columns are set to fit one training batch (32)\n",
    "n_rows = 8\n",
    "n_cols = 4\n",
    "plt.figure(figsize=(n_cols * 3, n_rows * 3))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range (n_rows*n_cols):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[labels[i]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=.2, hspace=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint to resume training if it stops unexpectedly\n",
    "checkpoint_path = \"../checkpoints/training_2/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size_1 = (224, 224)\n",
    "# train_ds_1 = train_ds.map(lambda image, label: (tf.image.resize(image, ds_size_1), label))\n",
    "# validation_ds_1 = validation_ds.map(lambda image, label: (tf.image.resize(image, ds_size_1), label))\n",
    "\n",
    "train_ds_1 = train_ds\n",
    "validation_ds_1 = validation_ds\n",
    "\n",
    "\n",
    "base_model_2 = keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "n_classes = 3\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# Rebuild top\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model_2.output)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "top_dropout_rate = 0.2\n",
    "x = tf.keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "# x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(3, activation=\"softmax\", name=\"pred\")(x) # match number of classes\n",
    "\n",
    "model_2 = keras.models.Model(inputs=base_model_2.input,\n",
    "                           outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6408 - accuracy: 0.7106\n",
      "Epoch 1: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 180s 519ms/step - loss: 0.6408 - accuracy: 0.7106 - val_loss: 0.8320 - val_accuracy: 0.6728\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.7407\n",
      "Epoch 2: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 176s 517ms/step - loss: 0.5639 - accuracy: 0.7407 - val_loss: 0.5668 - val_accuracy: 0.7432\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5555 - accuracy: 0.7489\n",
      "Epoch 3: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 510ms/step - loss: 0.5555 - accuracy: 0.7489 - val_loss: 0.5584 - val_accuracy: 0.7612\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5477 - accuracy: 0.7565\n",
      "Epoch 4: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 509ms/step - loss: 0.5477 - accuracy: 0.7565 - val_loss: 0.5395 - val_accuracy: 0.7748\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5459 - accuracy: 0.7594\n",
      "Epoch 5: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 511ms/step - loss: 0.5459 - accuracy: 0.7594 - val_loss: 0.5453 - val_accuracy: 0.7689\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5301 - accuracy: 0.7667\n",
      "Epoch 6: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 509ms/step - loss: 0.5301 - accuracy: 0.7667 - val_loss: 0.5318 - val_accuracy: 0.7718\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.7569\n",
      "Epoch 7: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.5432 - accuracy: 0.7569 - val_loss: 0.5396 - val_accuracy: 0.7722\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.7565\n",
      "Epoch 8: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 510ms/step - loss: 0.5381 - accuracy: 0.7565 - val_loss: 0.5420 - val_accuracy: 0.7726\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.7548\n",
      "Epoch 9: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 176s 516ms/step - loss: 0.5380 - accuracy: 0.7548 - val_loss: 0.5291 - val_accuracy: 0.7766\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.7572\n",
      "Epoch 10: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.5358 - accuracy: 0.7572 - val_loss: 0.5238 - val_accuracy: 0.7806\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.7655\n",
      "Epoch 11: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.5288 - accuracy: 0.7655 - val_loss: 0.5390 - val_accuracy: 0.7726\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.7631\n",
      "Epoch 12: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 176s 516ms/step - loss: 0.5286 - accuracy: 0.7631 - val_loss: 0.5250 - val_accuracy: 0.7803\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5314 - accuracy: 0.7607\n",
      "Epoch 13: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.5314 - accuracy: 0.7607 - val_loss: 0.5217 - val_accuracy: 0.7902\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5355 - accuracy: 0.7608\n",
      "Epoch 14: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.5355 - accuracy: 0.7608 - val_loss: 0.5228 - val_accuracy: 0.7755\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.7590\n",
      "Epoch 15: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 515ms/step - loss: 0.5347 - accuracy: 0.7590 - val_loss: 0.5252 - val_accuracy: 0.7850\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.7601\n",
      "Epoch 16: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 177s 520ms/step - loss: 0.5347 - accuracy: 0.7601 - val_loss: 0.5302 - val_accuracy: 0.7828\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.7626\n",
      "Epoch 17: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 511ms/step - loss: 0.5262 - accuracy: 0.7626 - val_loss: 0.5167 - val_accuracy: 0.7825\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.7669\n",
      "Epoch 18: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 511ms/step - loss: 0.5218 - accuracy: 0.7669 - val_loss: 0.5222 - val_accuracy: 0.7810\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.7652\n",
      "Epoch 19: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 511ms/step - loss: 0.5298 - accuracy: 0.7652 - val_loss: 0.5345 - val_accuracy: 0.7773\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.7600\n",
      "Epoch 20: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 513ms/step - loss: 0.5278 - accuracy: 0.7600 - val_loss: 0.5170 - val_accuracy: 0.7770\n"
     ]
    }
   ],
   "source": [
    "# train up the top layer first\n",
    "\n",
    "for layer in base_model_2.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01, decay=0.01)\n",
    "model_2.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model_2.fit(train_ds_1,\n",
    "                    validation_data=validation_ds_1,\n",
    "#                     class_weight=class_weights,\n",
    "                    epochs=20, callbacks=[callback,cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = tf.keras.optimizers.Adam(0.1)\n",
    "# net = Net()\n",
    "# dataset = toy_dataset()\n",
    "# iterator = iter(dataset)\n",
    "# ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=net, iterator=iterator)\n",
    "# manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "# train_and_checkpoint(net, manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.7362 - accuracy: 0.6940\n",
      "Epoch 1: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 179s 515ms/step - loss: 0.7362 - accuracy: 0.6940 - val_loss: 13.9143 - val_accuracy: 0.2384\n",
      "Epoch 2/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.7337\n",
      "Epoch 2: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 511ms/step - loss: 0.6195 - accuracy: 0.7337 - val_loss: 94.1256 - val_accuracy: 0.0888\n",
      "Epoch 3/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.7501\n",
      "Epoch 3: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 513ms/step - loss: 0.5934 - accuracy: 0.7501 - val_loss: 25.7096 - val_accuracy: 0.2384\n",
      "Epoch 4/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5596 - accuracy: 0.7636\n",
      "Epoch 4: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 176s 515ms/step - loss: 0.5596 - accuracy: 0.7636 - val_loss: 13.7972 - val_accuracy: 0.6728\n",
      "Epoch 5/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5412 - accuracy: 0.7725\n",
      "Epoch 5: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 513ms/step - loss: 0.5412 - accuracy: 0.7725 - val_loss: 9.7936 - val_accuracy: 0.6728\n",
      "Epoch 6/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.5216 - accuracy: 0.7793\n",
      "Epoch 6: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 513ms/step - loss: 0.5216 - accuracy: 0.7793 - val_loss: 10.5094 - val_accuracy: 0.6728\n",
      "Epoch 7/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.7867\n",
      "Epoch 7: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.4991 - accuracy: 0.7867 - val_loss: 3.1355 - val_accuracy: 0.7425\n",
      "Epoch 8/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.7909\n",
      "Epoch 8: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 513ms/step - loss: 0.4965 - accuracy: 0.7909 - val_loss: 131.3436 - val_accuracy: 0.0888\n",
      "Epoch 9/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4818 - accuracy: 0.7971\n",
      "Epoch 9: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.4818 - accuracy: 0.7971 - val_loss: 10.1449 - val_accuracy: 0.2392\n",
      "Epoch 10/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.8035\n",
      "Epoch 10: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 510ms/step - loss: 0.4786 - accuracy: 0.8035 - val_loss: 28.6757 - val_accuracy: 0.1012\n",
      "Epoch 11/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4604 - accuracy: 0.8065\n",
      "Epoch 11: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.4604 - accuracy: 0.8065 - val_loss: 32.2328 - val_accuracy: 0.2384\n",
      "Epoch 12/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.8076\n",
      "Epoch 12: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 176s 516ms/step - loss: 0.4584 - accuracy: 0.8076 - val_loss: 5.1630 - val_accuracy: 0.7175\n",
      "Epoch 13/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4464 - accuracy: 0.8152\n",
      "Epoch 13: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.4464 - accuracy: 0.8152 - val_loss: 9.5217 - val_accuracy: 0.6728\n",
      "Epoch 14/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4390 - accuracy: 0.8169\n",
      "Epoch 14: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 509ms/step - loss: 0.4390 - accuracy: 0.8169 - val_loss: 69.7493 - val_accuracy: 0.0888\n",
      "Epoch 15/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.8176\n",
      "Epoch 15: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 510ms/step - loss: 0.4345 - accuracy: 0.8176 - val_loss: 6.6940 - val_accuracy: 0.6731\n",
      "Epoch 16/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.8241\n",
      "Epoch 16: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 510ms/step - loss: 0.4270 - accuracy: 0.8241 - val_loss: 7.9420 - val_accuracy: 0.6728\n",
      "Epoch 17/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4169 - accuracy: 0.8301\n",
      "Epoch 17: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 513ms/step - loss: 0.4169 - accuracy: 0.8301 - val_loss: 19.8072 - val_accuracy: 0.2384\n",
      "Epoch 18/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.8257\n",
      "Epoch 18: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 172s 506ms/step - loss: 0.4188 - accuracy: 0.8257 - val_loss: 9.6366 - val_accuracy: 0.6728\n",
      "Epoch 19/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4214 - accuracy: 0.8240\n",
      "Epoch 19: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 173s 508ms/step - loss: 0.4214 - accuracy: 0.8240 - val_loss: 22.3759 - val_accuracy: 0.0895\n",
      "Epoch 20/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.8313\n",
      "Epoch 20: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 172s 505ms/step - loss: 0.4060 - accuracy: 0.8313 - val_loss: 6.7078 - val_accuracy: 0.7190\n",
      "Epoch 21/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4146 - accuracy: 0.8262\n",
      "Epoch 21: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.4146 - accuracy: 0.8262 - val_loss: 8.7872 - val_accuracy: 0.2384\n",
      "Epoch 22/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4057 - accuracy: 0.8264\n",
      "Epoch 22: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.4057 - accuracy: 0.8264 - val_loss: 3.8255 - val_accuracy: 0.6775\n",
      "Epoch 23/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3984 - accuracy: 0.8353\n",
      "Epoch 23: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 510ms/step - loss: 0.3984 - accuracy: 0.8353 - val_loss: 5.8046 - val_accuracy: 0.4468\n",
      "Epoch 24/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.8346\n",
      "Epoch 24: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.4014 - accuracy: 0.8346 - val_loss: 5.9620 - val_accuracy: 0.6728\n",
      "Epoch 25/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.8385\n",
      "Epoch 25: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 513ms/step - loss: 0.3925 - accuracy: 0.8385 - val_loss: 11.8514 - val_accuracy: 0.2388\n",
      "Epoch 26/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.4016 - accuracy: 0.8306\n",
      "Epoch 26: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.4016 - accuracy: 0.8306 - val_loss: 7.5485 - val_accuracy: 0.6742\n",
      "Epoch 27/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3964 - accuracy: 0.8350\n",
      "Epoch 27: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 511ms/step - loss: 0.3964 - accuracy: 0.8350 - val_loss: 36.8869 - val_accuracy: 0.0888\n",
      "Epoch 28/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.8360\n",
      "Epoch 28: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 510ms/step - loss: 0.3890 - accuracy: 0.8360 - val_loss: 7.7253 - val_accuracy: 0.1849\n",
      "Epoch 29/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3860 - accuracy: 0.8397\n",
      "Epoch 29: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.3860 - accuracy: 0.8397 - val_loss: 5.8428 - val_accuracy: 0.7043\n",
      "Epoch 30/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8379\n",
      "Epoch 30: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.3865 - accuracy: 0.8379 - val_loss: 8.2297 - val_accuracy: 0.6728\n",
      "Epoch 31/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8387\n",
      "Epoch 31: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.3828 - accuracy: 0.8387 - val_loss: 17.5288 - val_accuracy: 0.2663\n",
      "Epoch 32/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3879 - accuracy: 0.8350\n",
      "Epoch 32: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 510ms/step - loss: 0.3879 - accuracy: 0.8350 - val_loss: 7.4955 - val_accuracy: 0.2403\n",
      "Epoch 33/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8415\n",
      "Epoch 33: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 174s 509ms/step - loss: 0.3807 - accuracy: 0.8415 - val_loss: 3.1368 - val_accuracy: 0.6992\n",
      "Epoch 34/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3892 - accuracy: 0.8372\n",
      "Epoch 34: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 514ms/step - loss: 0.3892 - accuracy: 0.8372 - val_loss: 14.7340 - val_accuracy: 0.0928\n",
      "Epoch 35/50\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.8438\n",
      "Epoch 35: saving model to ../checkpoints/training_2/cp.ckpt\n",
      "341/341 [==============================] - 175s 512ms/step - loss: 0.3765 - accuracy: 0.8438 - val_loss: 7.3634 - val_accuracy: 0.6728\n",
      "Epoch 36/50\n",
      "141/341 [===========>..................] - ETA: 1:21 - loss: 0.3919 - accuracy: 0.8367"
     ]
    }
   ],
   "source": [
    "# train all the layers together for a bit with a much lower learning rate\n",
    "\n",
    "for layer in base_model_2.layers[-20:]:\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0004, decay=0.001)\n",
    "model_2.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model_2.fit(train_ds_1,\n",
    "                    validation_data=validation_ds_1,\n",
    "#                     class_weight=class_weights,\n",
    "                    epochs=50, callbacks=[callback,cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_2.save('saved_models/model_TEST') # change this path to save a new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you need to use the checkpoint, use this code\n",
    "# # source: https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_options\n",
    "\n",
    "# latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# latest\n",
    "\n",
    "# # Create a new model instance\n",
    "# model_2 = create_model()\n",
    "\n",
    "# # Load the previously saved weights\n",
    "# model_2.load_weights(latest)\n",
    "\n",
    "# # Re-evaluate the model\n",
    "# loss, acc = model_2.evaluate(validation_ds_1 verbose=2)\n",
    "# print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "Note this is model was trained on train/test data before validation (holdout data was taken out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_1 = model_1.predict(validation_ds_1, batch_size=batch_size)\n",
    "# len(predictions_1)\n",
    "\n",
    "# or get predeictions from saved model\n",
    "\n",
    "saved_model_1 = tf.keras.models.load_model('./saved_models/model_1')\n",
    "predictions_1 = saved_model_1.predict(validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x7f2531eb69d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m validation_ds:\n\u001b[1;32m      3\u001b[0m     y_true\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m      4\u001b[0m y_true\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py:104\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py:116\u001b[0m, in \u001b[0;36mIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py:227\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    225\u001b[0m filepaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[0;32m--> 227\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                   \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     x \u001b[38;5;241m=\u001b[39m img_to_array(img, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Pillow images should be closed after `load_img`,\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# but not PIL images.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:125\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m color_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor_mode must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgba\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/PIL/Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "for img, label in validation_ds:\n",
    "    y_true.append(label.numpy())\n",
    "y_true\n",
    "\n",
    "# create and plot confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(\n",
    "    np.concatenate(y_true),\n",
    "    np.argmax(predictions_1, axis=1),\n",
    "    num_classes=3,\n",
    "    weights=None) # change to get to 100% covid accuracy\n",
    "# confusion_mtx = confusion_mtx/confusion_mtx.numpy().sum(axis=1)[:, tf.newaxis]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale = 2)\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.savefig('matrix1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# HOLD_ds_1 = HOLD_ds.map(lambda image, label: (tf.image.resize(image, ds_size_1), label))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m HOLD_predictions_1 \u001b[38;5;241m=\u001b[39m \u001b[43msaved_model_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHOLD_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m HOLD_ds:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training.py:1982\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1981\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 1982\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1983\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1984\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# HOLD_ds_1 = HOLD_ds.map(lambda image, label: (tf.image.resize(image, ds_size_1), label))\n",
    "HOLD_predictions_1 = saved_model_1.predict(HOLD_ds)\n",
    "\n",
    "y_true = []\n",
    "for img, label in HOLD_ds:\n",
    "    y_true.append(label)\n",
    "y_true\n",
    "\n",
    "# create and plot confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(\n",
    "    np.concatenate(y_true),\n",
    "    np.argmax(HOLD_predictions_1, axis=1),\n",
    "    num_classes=3,\n",
    "    weights=None) # change to get to 100% covid accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale = 2)\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.savefig('matrix1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0\n",
    "This model was created in the resnet_v2 file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or get predeictions from saved model\n",
    "\n",
    "saved_model_0 = tf.keras.models.load_model('./saved_models/model_0')\n",
    "predictions_0 = saved_model_0.predict(validation_ds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for img, label in validation_ds_1:\n",
    "    y_true.append(label.numpy())\n",
    "y_true\n",
    "\n",
    "# create and plot confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(\n",
    "    np.concatenate(y_true),\n",
    "    np.argmax(predictions_0, axis=1),\n",
    "    num_classes=3,\n",
    "    weights=None) # change to get to 100% covid accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale = 2)\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.savefig('matrix1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_1 = model_1.predict(validation_ds_1, batch_size=batch_size)\n",
    "# len(predictions_1)\n",
    "\n",
    "# or get predeictions from saved model\n",
    "\n",
    "saved_model_2 = tf.keras.models.load_model('./saved_models/model_2')\n",
    "predictions_2 = saved_model_2.predict(validation_ds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for img, label in validation_ds_1:\n",
    "    y_true.append(label.numpy())\n",
    "y_true\n",
    "\n",
    "# create and plot confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(\n",
    "    np.concatenate(y_true),\n",
    "    np.argmax(predictions_2, axis=1),\n",
    "    num_classes=3,\n",
    "    weights=None) # change to get to 100% covid accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale = 2)\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.savefig('matrix1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Holdout Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for img, label in validation_ds_1:\n",
    "    y_true.append(label.numpy())\n",
    "y_true\n",
    "\n",
    "# create and plot confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(\n",
    "    np.concatenate(y_true),\n",
    "    np.argmax(predictions_2, axis=1),\n",
    "    num_classes=3,\n",
    "    weights=None) # change to get to 100% covid accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale = 2)\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.savefig('matrix1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "\n",
    "Oversampling/Data Augmentation:\n",
    "\n",
    "1. start a new file with clear labels, resampling, augmented data\n",
    "2. Train the model the same way\n",
    "3. Save model and create confusion matrix in this file (or seperate file)\n",
    "\n",
    "Prediction weights\n",
    "1. When predicting classes, change wieghts until we get 100% for covid cases\n",
    "2. Change to proportional CM instead of just numeric?\n",
    "\n",
    "Recall and F-score as metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.8.0/Keras Py3.9",
   "language": "python",
   "name": "tensorflow-2.8.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
