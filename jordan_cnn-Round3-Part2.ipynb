{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import random as python_random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.random.seed(42) # note that you must use the same seed to ensure consistentcy in your training/validation/testing\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET Round 3, Part 2\n",
    "This model has already been trained for x-ray data, now we retrain on data that has been upsapmpled (for small classes) and downsampled (for large classes) to balance classes.\n",
    "\n",
    "I've decided to get 5,000 records of each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid_img': 3249, 'Viral_img': 1211, 'Normal_img': 9174}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../dl_data/\"\n",
    "class_names = os.listdir(data_path)\n",
    "class_dist = {} # get the originial distribution of each class\n",
    "f_names = {} # get list of file paths per class\n",
    "for c in class_names:\n",
    "    class_dist[c] = len(os.listdir(data_path + c))\n",
    "    f_names[c] = os.listdir(data_path + c)\n",
    "class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID-2402.png',\n",
       " 'COVID-1270.png',\n",
       " 'COVID-3070.png',\n",
       " 'COVID-2019.png',\n",
       " 'COVID-2463.png',\n",
       " 'COVID-396.png',\n",
       " 'COVID-3605.png',\n",
       " 'COVID-256.png',\n",
       " 'COVID-1215.png',\n",
       " 'COVID-1649.png']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_names['Covid_img'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['COVID-1113.png', 'COVID-2058.png', 'COVID-3289.png',\n",
       "       'COVID-564.png', 'COVID-1134.png', 'COVID-3547.png',\n",
       "       'COVID-3072.png', 'COVID-348.png', 'COVID-47.png',\n",
       "       'COVID-1730.png', 'COVID-538.png', 'COVID-2400.png',\n",
       "       'COVID-1660.png', 'COVID-1077.png', 'COVID-282.png',\n",
       "       'COVID-2015.png', 'COVID-1445.png', 'COVID-2920.png',\n",
       "       'COVID-189.png', 'COVID-308.png'], dtype='<U14')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(f_names['Covid_img'], size=20, replace=True, p=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "# split train and test data\n",
    "\n",
    "test_names = {}\n",
    "train_names = {}\n",
    "for c in class_names:\n",
    "    length = len(f_names[c])\n",
    "    samp_len = math.floor(length*.20)\n",
    "    test = random.sample(f_names[c],samp_len)\n",
    "    \n",
    "    test_names[c] = test\n",
    "    train_names[c] = [x for x in f_names[c] if x not in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid_img': array(['COVID-3271.png', 'COVID-290.png', 'COVID-423.png', ...,\n",
       "        'COVID-1258.png', 'COVID-837.png', 'COVID-2096.png'], dtype='<U14'),\n",
       " 'Viral_img': array(['Viral Pneumonia-86.png', 'Viral Pneumonia-688.png',\n",
       "        'Viral Pneumonia-584.png', ..., 'Viral Pneumonia-1200.png',\n",
       "        'Viral Pneumonia-862.png', 'Viral Pneumonia-1210.png'],\n",
       "       dtype='<U24'),\n",
       " 'Normal_img': array(['Normal-8991.png', 'Normal-3846.png', 'Normal-3714.png', ...,\n",
       "        'Normal-4941.png', 'Normal-1147.png', 'Normal-5964.png'],\n",
       "       dtype='<U18')}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get samples\n",
    "sample_paths_test = {}\n",
    "for c in test_names:\n",
    "    sample_paths_test[c] = np.random.choice(test_names[c], size=1000, replace=True, p=None)\n",
    "sample_paths_test\n",
    "\n",
    "sample_paths_train = {}\n",
    "for c in train_names:\n",
    "    sample_paths_train[c] = np.random.choice(train_names[c], size=5000, replace=True, p=None)\n",
    "sample_paths_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for c in sample_paths_test:\n",
    "    print(len(sample_paths_test[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "for c in sample_paths_train:\n",
    "    print(len(sample_paths_train[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to, use this to make a new directory\n",
    "\n",
    "# os.mkdir('../sample_data_train')\n",
    "# for c in class_names:\n",
    "#     os.mkdir('../sample_data_train/' + c)\n",
    "# os.mkdir('../sample_data_test')\n",
    "# for c in class_names:\n",
    "#     os.mkdir('../sample_data_test/' + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dl_data/Covid_img\n",
      "../dl_data/Viral_img\n",
      "../dl_data/Normal_img\n",
      "../dl_data/Covid_img\n",
      "../dl_data/Viral_img\n",
      "../dl_data/Normal_img\n"
     ]
    }
   ],
   "source": [
    "# copy sampled files over\n",
    "new_path_train = '../sample_data_train/'\n",
    "\n",
    "for c in sample_paths_train:\n",
    "    print(data_path + c)\n",
    "    for i, p in enumerate(sample_paths_train[c]):\n",
    "        shutil.copyfile(data_path + c + '/' + p, new_path_train + c + '/' + str(i) + '_' + p)\n",
    "\n",
    "new_path_test = '../sample_data_test/'\n",
    "\n",
    "for c in sample_paths_test:\n",
    "    print(data_path + c)\n",
    "    for i, p in enumerate(sample_paths_test[c]):\n",
    "        shutil.copyfile(data_path + c + '/' + p, new_path_test + c + '/' + str(i) + '_' + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Covid_img': 5000, 'Viral_img': 5000, 'Normal_img': 5000}\n",
      "{'Covid_img': 1000, 'Viral_img': 1000, 'Normal_img': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Check that the new data has arrived\n",
    "new_class_dist_train = {} # get the originial distribution of each class\n",
    "for c in class_names:\n",
    "    new_class_dist_train[c] = len(os.listdir(new_path_train + c))\n",
    "print(new_class_dist_train)\n",
    "\n",
    "new_class_dist_test = {} # get the originial distribution of each class\n",
    "for c in class_names:\n",
    "    new_class_dist_test[c] = len(os.listdir(new_path_test + c))\n",
    "print(new_class_dist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you need to start over\n",
    "# for c in class_names:\n",
    "#     all_files = os.listdir(new_path_train + c)\n",
    "#     for f in all_files:\n",
    "#         os.remove(new_path_train + c+ '/' + f)\n",
    "        \n",
    "# # if you need to start over\n",
    "# for c in class_names:\n",
    "#     all_files = os.listdir(new_path_test + c)\n",
    "#     for f in all_files:\n",
    "#         os.remove(new_path_test + c+ '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if for some reason there are checkpoint files in there\n",
    "\n",
    "# for i in os.listdir('../dl_data/Covid_img/.ipynb_checkpoints'):\n",
    "#     os.remove('../dl_data/Covid_img/.ipynb_checkpoints/'+i)\n",
    "\n",
    "# os.listdir('../dl_data/Covid_img/.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.rmdir('../dl_data/Covid_img/.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 files belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 19:25:19.280050: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-02 19:25:21.083216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38397 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:87:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files \n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "\n",
    "# directories\n",
    "data_dir_test = \"../sample_data_test/\"\n",
    "data_dir_train = \"../sample_data_train/\"\n",
    "\n",
    "#### save out augmented data for visualization\n",
    "\n",
    "# ## first delete any existing files\n",
    "# aug_dir = '../augmented_data'\n",
    "# aug_files = os.listdir(aug_dir)\n",
    "# for f in aug_files:\n",
    "#     os.remove(aug_dir + '/' + f)\n",
    "\n",
    "    \n",
    "batch_size = 32;\n",
    "# IMPORTANT: Depends on what pre-trained model you choose, you will need to change these dimensions accordingly\n",
    "img_height = 224; \n",
    "img_width = 224;\n",
    "    \n",
    "    \n",
    "# Train Dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir_train,\n",
    "    seed = 42,\n",
    "    image_size= (img_height, img_width),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "# Test Dataset\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir_test,\n",
    "    seed = 42,\n",
    "    image_size= (img_height, img_width),\n",
    "    batch_size = batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint to resume training if it stops unexpectedly\n",
    "checkpoint_path = \"../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 19:28:23.844293: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n",
      "2022-05-02 19:28:29.086838: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - ETA: 0s - loss: 1.2361 - accuracy: 0.5393\n",
      "Epoch 1: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 48s 74ms/step - loss: 1.2361 - accuracy: 0.5393 - val_loss: 0.9576 - val_accuracy: 0.6010\n",
      "Epoch 2/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.8432 - accuracy: 0.6669\n",
      "Epoch 2: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.8429 - accuracy: 0.6669 - val_loss: 0.7304 - val_accuracy: 0.6760\n",
      "Epoch 3/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.6327 - accuracy: 0.7455\n",
      "Epoch 3: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.6324 - accuracy: 0.7454 - val_loss: 0.4986 - val_accuracy: 0.7777\n",
      "Epoch 4/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.4408 - accuracy: 0.8257\n",
      "Epoch 4: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.4407 - accuracy: 0.8257 - val_loss: 0.4362 - val_accuracy: 0.8097\n",
      "Epoch 5/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.8805\n",
      "Epoch 5: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.3179 - accuracy: 0.8807 - val_loss: 0.3422 - val_accuracy: 0.8577\n",
      "Epoch 6/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.2727 - accuracy: 0.9035\n",
      "Epoch 6: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.2727 - accuracy: 0.9035 - val_loss: 0.3790 - val_accuracy: 0.8543\n",
      "Epoch 7/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9248\n",
      "Epoch 7: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.2039 - accuracy: 0.9247 - val_loss: 0.3770 - val_accuracy: 0.8573\n",
      "Epoch 8/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.1588 - accuracy: 0.9437\n",
      "Epoch 8: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 34s 71ms/step - loss: 0.1587 - accuracy: 0.9437 - val_loss: 0.3624 - val_accuracy: 0.8657\n",
      "Epoch 9/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9563\n",
      "Epoch 9: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.1211 - accuracy: 0.9563 - val_loss: 0.5556 - val_accuracy: 0.8240\n",
      "Epoch 10/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9662\n",
      "Epoch 10: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.1024 - accuracy: 0.9661 - val_loss: 0.2750 - val_accuracy: 0.9057\n",
      "Epoch 11/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0800 - accuracy: 0.9737\n",
      "Epoch 11: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0802 - accuracy: 0.9736 - val_loss: 0.3503 - val_accuracy: 0.9127\n",
      "Epoch 12/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0544 - accuracy: 0.9825\n",
      "Epoch 12: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0544 - accuracy: 0.9825 - val_loss: 0.4355 - val_accuracy: 0.8923\n",
      "Epoch 13/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9841\n",
      "Epoch 13: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0486 - accuracy: 0.9841 - val_loss: 0.2765 - val_accuracy: 0.9120\n",
      "Epoch 14/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0431 - accuracy: 0.9857\n",
      "Epoch 14: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0431 - accuracy: 0.9857 - val_loss: 0.2223 - val_accuracy: 0.9283\n",
      "Epoch 15/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9892\n",
      "Epoch 15: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0327 - accuracy: 0.9893 - val_loss: 0.3244 - val_accuracy: 0.9100\n",
      "Epoch 16/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0249 - accuracy: 0.9923\n",
      "Epoch 16: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.2128 - val_accuracy: 0.9387\n",
      "Epoch 17/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9933\n",
      "Epoch 17: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0222 - accuracy: 0.9933 - val_loss: 0.3133 - val_accuracy: 0.9223\n",
      "Epoch 18/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9922\n",
      "Epoch 18: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.2617 - val_accuracy: 0.9297\n",
      "Epoch 19/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0233 - accuracy: 0.9929\n",
      "Epoch 19: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0233 - accuracy: 0.9929 - val_loss: 0.3072 - val_accuracy: 0.9297\n",
      "Epoch 20/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9937\n",
      "Epoch 20: saving model to ../checkpoints/training_ROUND3_part2_fixed/cp.ckpt\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0200 - accuracy: 0.9937 - val_loss: 0.2907 - val_accuracy: 0.9257\n"
     ]
    }
   ],
   "source": [
    "# train up the top layer first\n",
    "\n",
    "model_2 = tf.keras.models.load_model('./saved_models/model_ROUND3')\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# recall = tf.keras.metrics.Recall()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01, decay=0.01)\n",
    "model_2.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model_2.fit(train_ds,\n",
    "                    validation_data=validation_ds,\n",
    "#                     class_weight=class_weights,\n",
    "                    epochs=20, callbacks=[callback,cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_2.fit(train_ds_1,\n",
    "#                     validation_data=validation_ds_1,\n",
    "# #                     class_weight=class_weights,\n",
    "#                     epochs=1, callbacks=[callback,cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 19:39:46.177051: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/model_ROUND3_part2/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model_2.save('saved_models/model_ROUND3_part2') # change this path to save a new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.8.0/Keras Py3.9",
   "language": "python",
   "name": "tensorflow-2.8.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
